{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nimport re\nimport os\nimport random","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/questions-chapter-classification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install bert-for-tf2\n!pip install sentencepiece","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\n\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras import layers\nimport bert","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(columns=['q_id'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['class']=train['class'].map({6:0,7:1,8:2,9:3,10:4,11:5,12:6})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstop_words.extend(list(string.punctuation))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stopword_remove(sent):\n    tokens=word_tokenize(sent)\n    sentence=[token for token in tokens if token not in stop_words]\n    return ' '.join(sentence) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['eng']=train['eng'].apply(lambda sent:stopword_remove(sent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['chapter']=train['chapter'].apply(lambda sent:stopword_remove(sent))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FullTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n                            trainable=False)\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode_sentence(eng,chapter):\n    return [\"[CLS]\"] + tokenizer.tokenize(eng) + [\"[SEP]\"]+tokenizer.tokenize(chapter)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_inputs =[encode_sentence(eng,chapter) for eng,chapter in zip(train['eng'],train['chapter'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\ndef get_mask(tokens):\n    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n\ndef get_segments(tokens):\n    seg_ids = []\n    current_seg_id = 0\n    for tok in tokens:\n        seg_ids.append(current_seg_id)\n        if tok == \"[SEP]\":\n            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n    return seg_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_labels=train['class'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]\nrandom.shuffle(data_with_len)\ndata_with_len.sort(key=lambda x: x[2])\nsorted_all = [([get_ids(sent_lab[0]),\n                get_mask(sent_lab[0]),\n                get_segments(sent_lab[0])],\n               sent_lab[1])\n              for sent_lab in data_with_len if 512>sent_lab[2] > 3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list is a type of iterator so it can be used as generator for a dataset\nall_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types=(tf.int32, tf.int32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"next(iter(all_dataset))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE,\n                                       padded_shapes=((3, None), ()),\n                                       padding_values=(0, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\nNB_BATCHES_TEST = NB_BATCHES // 10\nall_batched.shuffle(NB_BATCHES)\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DCNNBERTEmbedding(tf.keras.Model):\n    \n    def __init__(self,\n                 nb_filters=50,\n                 FFN_units=512,\n                 nb_classes=7,\n                 dropout_rate=0.1,\n                 name=\"dcnn\"):\n        super(DCNNBERTEmbedding, self).__init__(name=name)\n        \n        self.bert_layer = hub.KerasLayer(\n            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n            trainable=False)\n\n        self.bigram = layers.Conv1D(filters=nb_filters,\n                                    kernel_size=2,\n                                    padding=\"valid\",\n                                    activation=\"relu\")\n        self.trigram = layers.Conv1D(filters=nb_filters,\n                                     kernel_size=3,\n                                     padding=\"valid\",\n                                     activation=\"relu\")\n        self.fourgram = layers.Conv1D(filters=nb_filters,\n                                      kernel_size=4,\n                                      padding=\"valid\",\n                                      activation=\"relu\")\n        self.pool = layers.GlobalMaxPool1D()\n        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n        self.dropout = layers.Dropout(rate=dropout_rate)\n        if nb_classes == 2:\n            self.last_dense = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.last_dense = layers.Dense(units=nb_classes,\n                                           activation=\"softmax\")\n    \n    def embed_with_bert(self, all_tokens):\n        _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                   all_tokens[:, 1, :],\n                                   all_tokens[:, 2, :]])\n        return embs\n\n    def call(self, inputs, training):\n        x = self.embed_with_bert(inputs)\n\n        x_1 = self.bigram(x)\n        x_1 = self.pool(x_1)\n        x_2 = self.trigram(x)\n        x_2 = self.pool(x_2)\n        x_3 = self.fourgram(x)\n        x_3 = self.pool(x_3)\n        \n        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n        merged = self.dense_1(merged)\n        merged = self.dropout(merged, training)\n        output = self.last_dense(merged)\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 7\n\nDROPOUT_RATE = 0.2\n\nBATCH_SIZE = 32\nNB_EPOCHS = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n                         FFN_units=FFN_UNITS,\n                         nb_classes=NB_CLASSES,\n                         dropout_rate=DROPOUT_RATE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if NB_CLASSES == 2:\n    Dcnn.compile(loss=\"binary_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])\nelse:\n    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=\"adam\",\n                 metrics=[\"accuracy\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Dcnn.fit(train_dataset,\n         epochs=NB_EPOCHS,validation_data=test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = Dcnn.evaluate(test_dataset)\nprint(results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val=pd.read_csv('../input/questions-chapter-classification/val.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val['class']=val['class'].map({6:0,7:1,8:2,9:3,10:4,11:5,12:6})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"val.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prediction(eng,chapter):\n    eng=stopword_remove(eng)\n    chapter=stopword_remove(chapter)\n    tokens = encode_sentence(eng,chapter)\n\n    input_ids = get_ids(tokens)\n    input_mask = get_mask(tokens)\n    segment_ids = get_segments(tokens)\n\n    inputs = tf.stack(\n        [tf.cast(input_ids, dtype=tf.int32),\n         tf.cast(input_mask, dtype=tf.int32),\n         tf.cast(segment_ids, dtype=tf.int32)],\n         axis=0)\n    inputs = tf.expand_dims(inputs, 0) # simulates a batch\n\n    output = Dcnn(inputs, training=False)\n    \n    output=tf.math.argmax(output,axis=-1)\n    \n    output=tf.keras.backend.get_value(output)\n\n    return output[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=[]\nfor eng,chapter in zip(val['eng'],val['chapter']):\n    pred.append(get_prediction(eng,chapter))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(pred,val['class']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(pred,val['class']))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}